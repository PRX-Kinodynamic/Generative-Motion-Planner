import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import numpy as np
from einops import rearrange

from .helpers import SinusoidalPosEmb


class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        Args:
            x: Tensor, shape [seq_len, batch_size, embedding_dim]
        """
        return x + self.pe[:x.size(0)]


class TimeEmbedding(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim
        self.time_mlp = nn.Sequential(
            SinusoidalPosEmb(dim // 4),
            nn.Linear(dim // 4, dim // 2),
            nn.GELU(),
            nn.Linear(dim // 2, dim),
        )

    def forward(self, t):
        """
        Args:
            t: Tensor, shape [batch_size]
        """
        return self.time_mlp(t)


class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)
        
        # Implementation of Feedforward model
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)

        # History integration components
        self.history_gate = nn.Linear(d_model * 2, d_model)
        self.history_projection = nn.Linear(d_model, d_model)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm_history = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

        self.activation = F.gelu

    def forward(self, src, history_embed=None, time_embed=None):
        # Add time embedding if provided
        x = src
        if time_embed is not None:
            # Reshape time embedding to match sequence dimension
            time_embed = time_embed.unsqueeze(1).expand(-1, src.size(1), -1)
            x = x + time_embed
            
        # Self attention block
        src2 = self.norm1(x)
        src2, _ = self.self_attn(src2, src2, src2)
        x = x + self.dropout1(src2)
        
        # Integrate history information directly if provided
        if history_embed is not None:
            # Process history embedding
            hist = self.norm_history(history_embed)
            
            # Create a history context vector through mean pooling
            # This reduces the time dimension of history
            hist_context = hist.mean(dim=1, keepdim=True).expand(-1, x.size(1), -1)
            
            # Combine with main features using a gating mechanism
            gate_input = torch.cat([x, hist_context], dim=-1)
            gate = torch.sigmoid(self.history_gate(gate_input))
            hist_features = self.history_projection(hist_context)
            
            # Apply gated integration of history features
            x = x + gate * hist_features
        
        # Feed forward block
        src2 = self.norm2(x)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src2))))
        x = x + self.dropout2(src2)
        
        return x


class PlanTransformer(nn.Module):
    """
    Transformer model for plan generation.
    Takes history observations as input and predicts future plans.
    Can be used for both direct prediction and flow matching.
    """
    
    def __init__(
        self,
        horizon,               # Length of prediction horizon
        output_dim,        # Dimension of the plan
        cond_dim,              # Dimension of history observations
        dim=256,               # Model width
        depth=6,               # Number of transformer layers
        heads=8,               # Number of attention heads
        dim_feedforward=512,   # Feedforward dimension
        dropout=0.1,           # Dropout rate
        cross_attention=True,  # Whether to use cross attention for history conditioning
    ):
        super().__init__()
        
        self.horizon = horizon
        self.output_dim = output_dim
        self.cond_dim = cond_dim
        self.dim = dim
        self.depth = depth
        
        # Time embedding - optional for direct prediction
        self.time_embed = TimeEmbedding(dim)
        
        # Input projections
        self.state_encoder = nn.Linear(output_dim, dim)
        self.history_encoder = nn.Linear(cond_dim, dim)
        
        # History context projection
        self.history_context_projector = nn.Sequential(
            nn.Linear(dim, dim),
            nn.LayerNorm(dim),
            nn.GELU(),
            nn.Linear(dim, dim)
        )
        
        # For direct prediction, we'll use a dummy input token
        self.start_token = nn.Parameter(torch.randn(1, 1, dim))
        
        # Positional encoding
        self.pos_encoder = PositionalEncoding(dim)
        
        # Transformer layers
        self.transformer_layers = nn.ModuleList([
            TransformerEncoderLayer(dim, heads, dim_feedforward, dropout)
            for _ in range(depth)
        ])
        
        # Cross-attention for history conditioning
        self.cross_attention = cross_attention
        if cross_attention:
            self.cross_attn_layers = nn.ModuleList([
                nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True)
                for _ in range(depth)
            ])
            self.cross_norm = nn.ModuleList([
                nn.LayerNorm(dim)
                for _ in range(depth)
            ])
        
        # Output projection
        self.out = nn.Linear(dim, output_dim)
        
    def forward(self, x=None, cond=None, t=None):
        """
        Args:
            x: Optional tensor for flow matching or direct prediction, shape [batch_size x horizon x output_dim]
               If None, assumes direct prediction mode
            cond: History observations, shape [batch_size x history_length x cond_dim]
            t: Optional time parameter, shape [batch_size]
        
        For direct prediction mode, x is ignored and a sequence of future states is predicted.
        For flow matching mode, x is transformed based on the vector field.
        """
        # Handle direct prediction case where x is None
        is_direct_prediction = x is None or (t is not None and torch.all(t == 0))
        
        if is_direct_prediction:
            return self.direct_prediction(cond)
        else:
            return self.flow_prediction(x, cond, t)
    
    def direct_prediction(self, history):
        """Direct prediction mode: predict future plan from history"""
        batch_size = history.shape[0]

        # Encode history
        history_embed = self.history_encoder(history)  # [batch_size x history_length x dim]
        history_embed = rearrange(history_embed, 'b t d -> t b d')
        history_embed = self.pos_encoder(history_embed)
        history_embed = rearrange(history_embed, 't b d -> b t d')
        
        # Process history to create an initial context
        history_context = self.history_context_projector(history_embed.mean(dim=1, keepdim=True))
        
        # Start with a learnable token, expanded to batch size, enhanced with history context
        x_embed = self.start_token.expand(batch_size, -1, -1)  # [batch_size x 1 x dim]
        x_embed = x_embed + history_context  # Directly inject history information
        
        # Autoregressive generation
        outputs = []
        
        for t in range(self.horizon):
            # Process through transformer layers
            for i, layer in enumerate(self.transformer_layers):
                # Pass both x_embed and history_embed to the transformer layer
                x_embed = layer(x_embed, history_embed)
                
                # Apply cross-attention with history
                if self.cross_attention and history is not None:
                    x_norm = self.cross_norm[i](x_embed)
                    attn_output, _ = self.cross_attn_layers[i](
                        query=x_norm,
                        key=history_embed,
                        value=history_embed
                    )
                    x_embed = x_embed + attn_output
            
            # Project to output dimension
            next_state = self.out(x_embed[:, -1:, :])  # [batch_size x 1 x output_dim]
            outputs.append(next_state)
            
            # Encode and append the predicted state for the next step
            next_state_embed = self.state_encoder(next_state)  # [batch_size x 1 x dim]
            x_embed = torch.cat([x_embed, next_state_embed], dim=1)
        
        # Concatenate all outputs
        plan = torch.cat(outputs, dim=1)  # [batch_size x horizon x output_dim]
        return plan
    
    def flow_prediction(self, x, history, time):
        """Flow matching mode: transform the input based on the vector field"""
        batch_size, horizon, _ = x.shape
        
        # Time embedding
        t_embed = self.time_embed(time)  # [batch_size x dim]
        
        # Encode plan and history
        x_embed = self.state_encoder(x)  # [batch_size x horizon x dim]
        
        # Apply positional encoding
        x_embed = rearrange(x_embed, 'b t d -> t b d')
        x_embed = self.pos_encoder(x_embed)
        x_embed = rearrange(x_embed, 't b d -> b t d')
        
        # Encode history
        history_embed = self.history_encoder(history)  # [batch_size x history_length x dim]
        history_embed = rearrange(history_embed, 'b t d -> t b d')
        history_embed = self.pos_encoder(history_embed)
        history_embed = rearrange(history_embed, 't b d -> b t d')
        
        # Process through transformer layers
        for i, layer in enumerate(self.transformer_layers):
            # Pass both x_embed and history_embed to each transformer layer
            x_embed = layer(x_embed, history_embed, t_embed)
            
            # Apply cross-attention with history if enabled
            if self.cross_attention and history is not None:
                x_norm = self.cross_norm[i](x_embed)
                attn_output, _ = self.cross_attn_layers[i](
                    query=x_norm,
                    key=history_embed,
                    value=history_embed
                )
                x_embed = x_embed + attn_output
        
        # Project back to plan dimension
        out = self.out(x_embed)
        
        return out


class PlanTransformerFlowMatching(nn.Module):
    """
    Wrapper for the PlanTransformer to be used with flow matching.
    Predicts vector fields for continuous normalizing flows.
    """
    
    def __init__(
        self,
        horizon,
        output_dim,
        cond_dim,
        dim=256,
        depth=6,
        heads=8,
        dim_feedforward=512,
        dropout=0.1,
        cross_attention=True,
    ):
        super().__init__()
        
        self.horizon = horizon
        self.output_dim = output_dim
        self.cond_dim = cond_dim
        
        # The main transformer model
        self.transformer = PlanTransformer(
            horizon=horizon,
            output_dim=output_dim,
            cond_dim=cond_dim,
            dim=dim,
            depth=depth,
            heads=heads,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            cross_attention=cross_attention,
        )
        
    def forward(self, x, cond=None, t=None, *args, **kwargs):
        """
        Predict the vector field (velocity) at point x and time t.
        For flow matching, this predicts the direction and magnitude of the flow.
        
        x : [ batch_size x horizon x output_dim ]
        cond : [ batch_size x history_length x cond_dim ]
        t : [ batch_size ]
        """
        return self.transformer(x=x, cond=cond, t=t)
