from os import cpu_count, path, listdir
from random import shuffle
from typing import Callable, List, Union, Sequence, Optional
import matplotlib.pyplot as plt
from tqdm import tqdm
import torch
import numpy as np

from genMoPlan.models.generative.base import GenerativeModel
from genMoPlan.utils import to_torch, get_normalizer_params, get_data_trajectories_path

def _generate_trajectory_batch(
        start_states: np.ndarray,
        model: GenerativeModel,
        model_args: dict,
        device: str,
        max_path_length: Optional[int] = None,
        num_inference_steps: Optional[int] = None,
        conditional_sample_kwargs: dict = {}, 
        only_return_final_states: bool = False, 
        verbose: bool = True, 
        horizon_length: int = None,
    ):
    """
    Generate a trajectory from the model given the start states.

    TODO: Currently assumes that the conditions are only the initial states. This should be changed to support the history_length > 1 case and other conditioning methods as well as global and local queries.
    """

    batch_size = len(start_states)

    current_states = to_torch(start_states, dtype=torch.float32, device=device)

    current_idx = model_args.history_length
    prediction_length = horizon_length if horizon_length is not None else model_args.horizon_length

    if num_inference_steps is None:
        num_inference_steps = np.ceil((max_path_length - model_args.history_length) / prediction_length)
    else:
        max_path_length = model_args.history_length + num_inference_steps * prediction_length

    if not only_return_final_states:
        trajectories = np.zeros((batch_size, max_path_length, model_args.observation_dim))
        trajectories[:, 0] = np.array(start_states)
    else:
        trajectories = None
    
    with tqdm(total=num_inference_steps, disable=not verbose) as pbar:
        while current_idx < max_path_length:
            slice_path_length = min(prediction_length, max_path_length - current_idx)

            conditions = {0: current_states}

            sample = model(
                cond=conditions, 
                verbose=False, 
                return_chain=False, 
                **conditional_sample_kwargs
            )
            next_trajs = sample.trajectories[:, model_args.history_length: model_args.history_length + slice_path_length]

            # Adding the next states to the trajectory
            if not only_return_final_states:
                trajectories[:, current_idx: current_idx + slice_path_length] = next_trajs.cpu().numpy()

            current_states = next_trajs[:, -1]
            current_idx += slice_path_length

            # Free memory
            del next_trajs
            if device == "cuda" and hasattr(torch, 'cuda') and torch.cuda.is_available():
                torch.cuda.empty_cache()

            pbar.update(1)

    if only_return_final_states:
        return current_states.cpu().detach().numpy()

    return trajectories


def process_angles(data, angle_indices = None):
    """
    Process the angles of the data to normalize them to [-pi, pi] range
    """
    if angle_indices is None:
        raise ValueError("angle_indices must be provided")
    
    data = data.copy()

    for idx in angle_indices:
        # First ensure angles are in [0, 2pi] range
        data[..., idx] = np.mod(data[..., idx], 2 * np.pi)
        # Then convert from [0, 2pi] to [-pi, pi]
        data[..., idx][data[..., idx] > np.pi] -= 2 * np.pi

    return data

def generate_trajectories(
    model, 
    model_args, 
    unnormalized_start_states, 
    device,
    verbose: bool = True, 
    batch_size: int = 5000, 
    max_path_length: Optional[int] = None,
    num_inference_steps: Optional[int] = None,
    conditional_sample_kwargs: dict = {}, 
    only_return_final_states: bool = False, 
    post_process_fns: List[Callable] = [], 
    post_process_fn_kwargs: dict = {},
    horizon_length: int = None,
):
    """
    Generate a trajectory from the model given the start states.

    Args:
        model: The model to generate the trajectory from.
        model_args: The arguments used to generate the model.
        unnormalized_start_states: The initial states to start the trajectory from. These are un-normalized and will be normalized before passing to the model.
            batch_size x observation_dim
        max_path_length: The maximum length of the trajectory to generate.
        verbose: If True, print progress.
        batch_size: The batch size to use for generating the trajectories.
        only_return_final_states: If True, only return the final states of the trajectories.

    Returns:
        trajectories: The trajectories generated by the model.
        final_states: The final states of the trajectories.
    """
    from genMoPlan.datasets.normalization import get_normalizer, Normalizer

    assert type(max_path_length) != type(num_inference_steps), "Only one of max_path_length or num_inference_steps must be provided"

    if model_args.trajectory_normalizer is not None:
        normalizer: Normalizer = get_normalizer(model_args.trajectory_normalizer, get_normalizer_params(model_args))

        start_states = normalizer(unnormalized_start_states)
    else:
        start_states = unnormalized_start_states
        normalizer = None

    if only_return_final_states:
        final_states = np.zeros_like(start_states)
    else:
        trajectories = []

    if verbose:
        import math
        total_num_batches = math.ceil(len(start_states) / batch_size)

    for idx in range(0, len(start_states), batch_size):
        if verbose:
            current_batch = math.ceil(idx / batch_size)

            print(f"[ utils/trajectory ] Generating trajectories for batch {current_batch + 1}/{total_num_batches}" if total_num_batches > 1 else "[ utils/trajectory ] Generating trajectories")

        batch_start_states = start_states[idx: idx + batch_size]

        results = _generate_trajectory_batch(
            batch_start_states, 
            model, 
            model_args, 
            device,
            max_path_length=max_path_length,
            num_inference_steps=num_inference_steps,
            conditional_sample_kwargs=conditional_sample_kwargs, 
            only_return_final_states=only_return_final_states, 
            verbose=verbose, 
            horizon_length=horizon_length
        )

        if only_return_final_states:
            final_states[idx:idx+batch_size] = results
        else:
            trajectories.append(results)
            
        # Free memory
        if device == "cuda" and hasattr(torch, 'cuda') and torch.cuda.is_available():
            torch.cuda.empty_cache()

    if not only_return_final_states:
        trajectories = np.concatenate(trajectories, axis=0)
    
    if only_return_final_states:
        return process_states(final_states, normalizer, post_process_fns, post_process_fn_kwargs, verbose)
    
    return process_trajectories(trajectories, normalizer, post_process_fns, post_process_fn_kwargs, verbose)


def process_states(states, normalizer, post_process_fns, post_process_fn_kwargs, verbose=False):
    """
    Process the states
    - Un-normalize the states
    - Move the states from [-2pi, 2pi] to [-pi, pi]
    """
    if normalizer is not None:
        states = normalizer.unnormalize(states)

    if post_process_fns is not None:
        for post_process_fn in post_process_fns:
            states = post_process_fn(states, **post_process_fn_kwargs)

    return states


def process_trajectories(trajectories, normalizer, post_process_fns, post_process_fn_kwargs, verbose=False):
    """
    Process the trajectories
    - Un-normalize the trajectories
    - Move the trajectories from [-2pi, 2pi] to [-pi, pi]
    """
    if verbose:
        print("[ utils/trajectory ] Processing trajectories")
    
    if normalizer is not None:
        # Apply unnormalization to all trajectories at once
        all_trajectories = np.concatenate(trajectories, axis=0)
        all_trajectories = normalizer.unnormalize(all_trajectories)

        processed_trajectories = []

        # Split the processed trajectories back into individual trajectories
        traj_start_idx = 0
        traj_lengths = [len(traj) for traj in trajectories]

        for i, traj_length in enumerate(traj_lengths):
            traj_end_idx = traj_start_idx + traj_length
            processed_trajectories.append(all_trajectories[traj_start_idx:traj_end_idx])
            traj_start_idx += traj_length

        trajectories = np.array(processed_trajectories)

    if post_process_fns is not None:
        for post_process_fn in post_process_fns:
            trajectories = post_process_fn(trajectories, **post_process_fn_kwargs)

    return trajectories


def plot_trajectories(
    trajectories: Union[np.ndarray, Sequence[np.ndarray]],
    image_path: Optional[str] = None,
    verbose: bool = False,
    comparison_trajectories: Union[np.ndarray, Sequence[np.ndarray], None] = None,
    show_traj_ends: bool = False,
    return_plot: bool = False,
):
    """
    Visualize 2-D trajectories.

    The function now supports both a single ``numpy.ndarray`` of shape
    (n_trajectories, length, dim) as well as a ``List`` of numpy arrays of
    shape (length_i, dim). Handling the list directly avoids an unnecessary
    concatenation when the caller already maintains the trajectories in that
    format.
    """

    # Helper that flattens trajectories and extracts start/end points while
    # supporting both list- and ndarray-based inputs.
    def _flatten(trajs):
        if trajs is None:
            return None, None, None

        # If we already have a single ndarray, life is easy.
        if isinstance(trajs, np.ndarray):
            n, length, dim = trajs.shape
            flat = trajs.reshape(-1, dim)
            starts = trajs[:, 0]
            ends = trajs[:, -1]
            return flat, starts, ends

        # Otherwise assume a sequence (e.g. list) of ndarrays with matching dim.
        dim = trajs[0].shape[-1]
        flat = np.concatenate(trajs, axis=0)
        starts = np.array([t[0] for t in trajs])
        ends = np.array([t[-1] for t in trajs])
        return flat, starts, ends

    traj_flat, traj_starts, traj_ends = _flatten(trajectories)
    if traj_flat is None:
        raise ValueError("`trajectories` cannot be None")

    # Plot generated trajectories
    plt.scatter(
        traj_flat[:, 0],
        traj_flat[:, 1],
        s=0.1,
        color="black",
        alpha=1,
        marker=".",
        label="Generated Trajectories",
    )

    if show_traj_ends:
        plt.scatter(
            traj_starts[:, 0],
            traj_starts[:, 1],
            s=10,
            color="black",
            alpha=1,
            marker="s",
        )
        plt.scatter(
            traj_ends[:, 0],
            traj_ends[:, 1],
            s=10,
            color="black",
            alpha=1,
            marker="x",
        )

    if comparison_trajectories is not None:
        comp_flat, comp_starts, comp_ends = _flatten(comparison_trajectories)
        plt.scatter(
            comp_flat[:, 0],
            comp_flat[:, 1],
            s=0.1,
            color="red",
            alpha=1,
            marker="1",
            label="Ground Truth Trajectories",
        )
        if show_traj_ends:
            plt.scatter(
                comp_starts[:, 0],
                comp_starts[:, 1],
                s=10,
                color="red",
                alpha=1,
                marker="s",
            )
            plt.scatter(
                comp_ends[:, 0],
                comp_ends[:, 1],
                s=10,
                color="red",
                alpha=1,
                marker="x",
            )

    if image_path is not None:
        plt.savefig(image_path)
        if verbose:
            print(f"[ utils/trajectory ] Trajectories saved at {image_path}")

    if return_plot:
        return plt

    plt.close()



def get_fnames_to_load(dataset_path, trajectories_path, num_trajs=None, load_reverse=False):
    indices_fpath = path.join(dataset_path, "shuffled_indices.txt")

    if path.exists(indices_fpath):
        with open(indices_fpath, "r") as f:
            fnames = f.readlines()
            fnames = [f.strip() for f in fnames]

    else:
        print(f"[ utils/trajectory ] Could not find shuffled indices at {indices_fpath}. Generating new shuffled indices")
        all_fnames = listdir(trajectories_path)
        fnames = all_fnames.copy()
        shuffle(fnames)

        with open(indices_fpath, "w") as f:
            for fname in fnames:
                f.write(fname + "\n")

    if num_trajs is not None:
        if not load_reverse:
            fnames = fnames[:num_trajs]
        else:
            fnames = fnames[-num_trajs:]

    return fnames



def _read_trajectories_from_fpaths(read_trajectory_fn, trajectories_path, fnames, parallel=True):
    fpaths = [path.join(trajectories_path, fname) for fname in fnames]
    if not parallel:
        trajectories = []
        for fpath in tqdm(fpaths):
            if not fpath.endswith(".txt"):
                continue
            trajectories.append(read_trajectory_fn(fpath))
    else:
        import multiprocessing as mp

        with mp.Pool(cpu_count()) as pool:
            trajectories = list(
                tqdm(pool.imap(read_trajectory_fn, fpaths), total=len(fpaths))
            )

    trajectories = [trajectory for trajectory in trajectories if trajectory is not None]

    return trajectories

def load_trajectories(dataset, read_trajectory_fn, dataset_size=None, parallel=True, fnames=None, load_reverse=False) -> List[np.ndarray]:
    """
    load dataset from directory
    """
    dataset_path = path.join(get_data_trajectories_path(), dataset)
    trajectories_path = path.join(dataset_path, "trajectories")

    if fnames is None:
        fnames = get_fnames_to_load(dataset_path, trajectories_path, dataset_size, load_reverse)

    trajectories = []

    print(f"[ datasets/sequence ] Loading trajectories from {trajectories_path}")

    trajectories = _read_trajectories_from_fpaths(read_trajectory_fn, trajectories_path, fnames, parallel=parallel)
    trajectories = [np.array(trajectory, dtype=np.float32) for trajectory in trajectories if trajectory is not None]

    return trajectories


def get_trajectory_attractor_labels(final_states: np.ndarray, attractors: dict, attractor_dist_threshold: float, invalid_label: int = -1, verbose: bool = True):
    if verbose:
        print("[ utils/trajectory ] Getting attractor labels for trajectories")

    attractor_states = attractors.keys()
    attractor_states = np.array(list(attractor_states))

    attractor_labels = attractors.values()
    attractor_labels = np.array(list(attractor_labels))
    attractor_labels = attractor_labels.reshape(-1, 1)

    # Compute the distance between the final states and each of the attractors
    distances = np.linalg.norm(final_states[:, None] - attractor_states, axis=2)

    min_distance = np.min(distances, axis=1)
    min_distance_idx = np.argmin(distances, axis=1)

    predicted_labels = np.zeros_like(min_distance)

    predicted_labels[min_distance <= attractor_dist_threshold] = attractor_labels[min_distance_idx[min_distance < attractor_dist_threshold]].flatten()
    predicted_labels[min_distance > attractor_dist_threshold] = invalid_label

    return predicted_labels

