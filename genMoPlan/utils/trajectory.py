from functools import partial
from tqdm import tqdm
import matplotlib.pyplot as plt
from os import cpu_count, path, listdir
from random import shuffle
from typing import Callable, List
import torch
import numpy as np

from genMoPlan.models.generative.base import GenerativeModel
from genMoPlan.utils.arrays import to_torch
from genMoPlan.utils.model import get_normalizer_params

def _generate_trajectory_batch(
        start_states: np.ndarray, 
        model: GenerativeModel, 
        model_args: dict, 
        max_path_length: int, 
        conditional_sample_kwargs: dict = {}, 
        only_return_final_states: bool = False, 
        verbose: bool = True, 
        horizon_length: int = None,
    ):

    batch_size = len(start_states)

    current_states = to_torch(start_states, dtype=torch.float32, device=model_args.device)

    current_idx = model_args.history_length
    prediction_length = horizon_length if horizon_length is not None else model_args.horizon_length

    if not only_return_final_states:
        trajectories = np.zeros((batch_size, max_path_length, model_args.observation_dim))
        trajectories[:, 0] = np.array(start_states)
    else:
        trajectories = None
    
    with tqdm(total=max_path_length - model_args.history_length, disable=not verbose) as pbar:
        while current_idx < max_path_length:
            conditions = {0: current_states}

            next_trajs = model.forward(conditions, verbose=False, return_chain=False, **conditional_sample_kwargs).trajectories

            slice_path_length = min(prediction_length, max_path_length - current_idx)
            next_trajs = next_trajs[:, model_args.history_length: model_args.history_length + slice_path_length]

            # Adding the next states to the trajectory
            if not only_return_final_states:
                trajectories[:, current_idx: current_idx + slice_path_length] = next_trajs.cpu().numpy()

            current_states = next_trajs[:, -1]
            current_idx += slice_path_length

            # Free memory
            del next_trajs
            if next(model.parameters()).device.type == "cuda" and hasattr(torch, 'cuda') and torch.cuda.is_available():
                torch.cuda.empty_cache()

            pbar.update(slice_path_length)

    if only_return_final_states:
        return current_states.cpu().detach().numpy()

    return trajectories


def process_angles(data, angle_indices = None):
    """
    Process the angles of the data to normalize them to [-pi, pi] range
    """
    if angle_indices is None:
        raise ValueError("angle_indices must be provided")

    for idx in angle_indices:
        # First ensure angles are in [0, 2pi] range
        data[..., idx] = np.mod(data[..., idx], 2 * np.pi)
        # Then convert from [0, 2pi] to [-pi, pi]
        data[..., idx][data[..., idx] > np.pi] -= 2 * np.pi

    return data

def generate_trajectories(
    model, 
    model_args, 
    unnormalized_start_states, 
    max_path_length: int, 
    verbose: bool = True, 
    batch_size: int = 5000, 
    conditional_sample_kwargs: dict = {}, 
    only_return_final_states: bool = False, 
    post_process_fns: List[Callable] = [], 
    post_process_fn_kwargs: dict = {},
    horizon_length: int = None,
):
    from genMoPlan.datasets.normalization import get_normalizer, Normalizer
    """
    Generate a trajectory from the model given the start states.

    Args:
        model: The model to generate the trajectory from.
        model_args: The arguments used to generate the model.
        unnormalized_start_states: The initial states to start the trajectory from. These are un-normalized and will be normalized before passing to the model.
            batch_size x observation_dim
        max_path_length: The maximum length of the trajectory to generate.
        verbose: If True, print progress.
        batch_size: The batch size to use for generating the trajectories.
        only_return_final_states: If True, only return the final states of the trajectories.

    Returns:
        trajectories: The trajectories generated by the model.
        final_states: The final states of the trajectories.
    """
    if model_args.trajectory_normalizer is not None:
        normalizer: Normalizer = get_normalizer(model_args.trajectory_normalizer, get_normalizer_params(model_args))

        start_states = normalizer.normalize(unnormalized_start_states)
    else:
        start_states = unnormalized_start_states
        normalizer = None

    if only_return_final_states:
        final_states = np.zeros_like(start_states)
    else:
        trajectories = []

    if verbose:
        import math
        total_num_batches = math.ceil(len(start_states) / batch_size)

    for idx in range(0, len(start_states), batch_size):
        if verbose:
            current_batch = math.ceil(idx / batch_size)

            print(f"[ utils/trajectory ] Generating trajectories for batch {current_batch + 1}/{total_num_batches}" if total_num_batches > 1 else f"[ utils/trajectory ] Generating trajectories")

        batch_start_states = start_states[idx: idx + batch_size]

        results = _generate_trajectory_batch(
            batch_start_states, 
            model, 
            model_args, 
            max_path_length, 
            conditional_sample_kwargs=conditional_sample_kwargs, 
            only_return_final_states=only_return_final_states, 
            verbose=verbose, 
            horizon_length=horizon_length
        )

        if only_return_final_states:
            final_states[idx:idx+batch_size] = results
        else:
            trajectories.append(results)
            
        # Free memory
        if next(model.parameters()).device.type == "cuda" and hasattr(torch, 'cuda') and torch.cuda.is_available():
            torch.cuda.empty_cache()

    if not only_return_final_states:
        trajectories = np.concatenate(trajectories, axis=0)
    
    if only_return_final_states:
        return process_states(final_states, normalizer, post_process_fns, post_process_fn_kwargs, verbose)

    return process_trajectories(trajectories, normalizer, post_process_fns, post_process_fn_kwargs, verbose)


def process_states(states, normalizer, post_process_fns, post_process_fn_kwargs, verbose=False):
    """
    Process the states
    - Un-normalize the states
    - Move the states from [-2pi, 2pi] to [-pi, pi]
    """
    if normalizer is not None:
        states = normalizer.unnormalize(states)

    if post_process_fns is not None:
        for post_process_fn in post_process_fns:
            states = post_process_fn(states, **post_process_fn_kwargs)

    return states


def process_trajectories(trajectories, normalizer, post_process_fns, post_process_fn_kwargs, verbose=False):
    """
    Process the trajectories
    - Un-normalize the trajectories
    - Move the trajectories from [-2pi, 2pi] to [-pi, pi]
    """
    if verbose:
        print("[ utils/trajectory ] Processing trajectories")
    
    if normalizer is not None:
        # Apply unnormalization to all trajectories at once
        all_trajectories = np.concatenate(trajectories, axis=0)
        all_trajectories = normalizer.unnormalize(all_trajectories)
    
        processed_trajectories = []

        # Split the processed trajectories back into individual trajectories
        traj_start_idx = 0
        traj_lengths = [len(traj) for traj in trajectories]

        for i, traj_length in enumerate(traj_lengths):
            traj_end_idx = traj_start_idx + traj_length
            processed_trajectories.append(all_trajectories[traj_start_idx:traj_end_idx])

        trajectories = np.array(processed_trajectories)

    if post_process_fns is not None:
        for post_process_fn in post_process_fns:
            trajectories = post_process_fn(trajectories, **post_process_fn_kwargs)
    
    return trajectories


def plot_trajectories(trajectories, image_path=None, verbose=False, comparison_trajectories=None, show_traj_ends=False, return_plot=False):
    """
    Visualize the trajectories generated by the model.
    """

    # Reshape trajectories from n x length x dim to (n x length) x dim
    n, length, dim = trajectories.shape
    trajectories_reshaped = trajectories.reshape(-1, dim)

    plt.scatter(
        trajectories_reshaped[:, 0],
        trajectories_reshaped[:, 1],
        s=0.1,
        color="black",
        alpha=1,
        marker=".",
        label="Generated Trajectories",
    )

    if show_traj_ends:
        plt.scatter(
            trajectories[:, 0, 0],
            trajectories[:, 0, 1],
            s=10,
            color="black",
            alpha=1,
            marker="s",
        )

        plt.scatter(
            trajectories[:, -1, 0],
            trajectories[:, -1, 1],
            s=10,
            color="black",
            alpha=1,
            marker="x",
        )
    if comparison_trajectories is not None:
        comparison_trajectories_reshaped = comparison_trajectories.reshape(-1, dim)

        plt.scatter(
            comparison_trajectories_reshaped[:, 0],
            comparison_trajectories_reshaped[:, 1],
            s=0.1,
            color="red",
            alpha=1,
            marker="1",
            label="Ground Truth Trajectories",
        )

        if show_traj_ends:
            plt.scatter(
                comparison_trajectories[:, 0, 0],
                comparison_trajectories[:, 0, 1],
                s=10,
                color="red",
                alpha=1,
                marker="s",
            )

            plt.scatter(
                comparison_trajectories[:, -1, 0],
                comparison_trajectories[:, -1, 1],
                s=10,
                color="red",
                alpha=1,
                marker="x",
            )

    if image_path is not None:
        plt.savefig(image_path)
        if verbose:
            print(f"[ utils/trajectory ] Trajectories saved at {image_path}")

    if return_plot:
        return plt
    
    plt.close()



def get_fnames_to_load(dataset_path, trajectories_path, num_trajs=None, load_reverse=False):
    indices_fpath = path.join(dataset_path, "shuffled_indices.txt")

    if path.exists(indices_fpath):
        with open(indices_fpath, "r") as f:
            fnames = f.readlines()
            fnames = [f.strip() for f in fnames]

    else:
        print(f"[ utils/trajectory ] Could not find shuffled indices at {indices_fpath}. Generating new shuffled indices")
        all_fnames = listdir(trajectories_path)
        fnames = all_fnames.copy()
        shuffle(fnames)

        with open(indices_fpath, "w") as f:
            for fname in fnames:
                f.write(fname + "\n")

    if num_trajs is not None:
        if not load_reverse:
            fnames = fnames[:num_trajs]
        else:
            fnames = fnames[-num_trajs:]

    return fnames


def _read_trajectory(delimiter, observation_dim, sequence_path, ignore_empty_lines=False):
    with open(sequence_path, "r") as f:
        lines = f.readlines()

    trajectory = []

    for i, line in enumerate(lines):
        line = line.strip()
        if line == "":
            if i < len(lines) - 1 and not ignore_empty_lines:
                raise ValueError(f"[ utils/trajectory ] Empty line found at {sequence_path} at line {i}")
            elif ignore_empty_lines:
                continue
            else:
                break

        state = line.split(delimiter)

        state = [s for s in state if s != ""]

        if observation_dim is not None:
            if len(state) < observation_dim:
                raise ValueError(f"[ utils/trajectory ] Trajectory at {sequence_path} has less than {observation_dim} states at line {i}")

            state = state[:observation_dim]

        state = [float(s) for s in state]

        trajectory.append(state)

    return np.array(trajectory, dtype=np.float32)


def read_trajectories_from_fpaths(trajectories_path, fnames, observation_dim: int, parallel=True, delimiter=",", ignore_empty_lines=False):
    fpaths = [path.join(trajectories_path, fname) for fname in fnames]
    if not parallel:
        trajectories = []
        for fpath in tqdm(fpaths):
            if not fpath.endswith(".txt"):
                continue
            trajectories.append(_read_trajectory(delimiter, observation_dim, fpath, ignore_empty_lines))
    else:
        import multiprocessing as mp

        args_list = [(delimiter, observation_dim, fpath, ignore_empty_lines) for fpath in fpaths]

        with mp.Pool(cpu_count()) as pool:
            trajectories = list(
                tqdm(pool.starmap(_read_trajectory, args_list), total=len(args_list))
            )

    return trajectories

def load_trajectories(dataset, observation_dim: int, dataset_size=None, parallel=True, fnames=None, load_reverse=False):
    """
    load dataset from directory
    """
    dataset_path = path.join("data_trajectories", dataset)
    trajectories_path = path.join(dataset_path, "trajectories")

    if fnames is None:
        fnames = get_fnames_to_load(dataset_path, trajectories_path, dataset_size, load_reverse)

    trajectories = []

    print(f"[ datasets/sequence ] Loading trajectories from {trajectories_path}")

    trajectories = read_trajectories_from_fpaths(trajectories_path, fnames, observation_dim, parallel)
    trajectories = np.array(trajectories, dtype=np.float32)

    return trajectories


def get_trajectory_attractor_labels(final_states: np.ndarray, attractors: dict, attractor_dist_threshold: float, invalid_label: int = -1, verbose: bool = True):
    if verbose:
        print("[ utils/trajectory ] Getting attractor labels for trajectories")

    attractor_states = attractors.keys()
    attractor_states = np.array(list(attractor_states))

    attractor_labels = attractors.values()
    attractor_labels = np.array(list(attractor_labels))
    attractor_labels = attractor_labels.reshape(-1, 1)

    # Compute the distance between the final states and each of the attractors
    distances = np.linalg.norm(final_states[:, None] - attractor_states, axis=2)

    min_distance = np.min(distances, axis=1)
    min_distance_idx = np.argmin(distances, axis=1)

    predicted_labels = np.zeros_like(min_distance)

    predicted_labels[min_distance <= attractor_dist_threshold] = attractor_labels[min_distance_idx[min_distance < attractor_dist_threshold]].flatten()
    predicted_labels[min_distance > attractor_dist_threshold] = invalid_label

    return predicted_labels

